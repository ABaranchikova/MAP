{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = pd.read_csv('ml_data.csv', encoding = 'utf-8') #пока наилучший вариант был с третьей итерацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = texts.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = texts[texts.in_out.isin(['0', '1'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts['id'] = texts['id'].factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(texts, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = train['in_out'] \n",
    "y_test = test['in_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyphen import Pyphen \n",
    "import string\n",
    "exclude = list(string.punctuation)\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sent_list = re.split(' *[\\.\\?!][\\'\"\\)\\]]* ', text)\n",
    "    return sent_list\n",
    " \n",
    "def text_len_sent(text):\n",
    "    TL_sent = len(sentence_splitter(text))\n",
    "    return TL_sent\n",
    "    \n",
    "def text_len_words(text):\n",
    "    TL_words = len(text.split())\n",
    "    return TL_words\n",
    " \n",
    "def avg_sentence_length(text):\n",
    "    ASL = float(text_len_words(text)/text_len_sent(text))\n",
    "    return round(ASL, 2)\n",
    "    \n",
    "    \n",
    "def avg_sent_per_word(text):\n",
    "    ASPW = float(text_len_sent(text)/text_len_words(text))\n",
    "    return round(ASPW, 2)\n",
    "    \n",
    "    \n",
    "def char_count(text, ignore_spaces=True):\n",
    "    if ignore_spaces:\n",
    "        text_chars = text.replace(\" \", \"\")\n",
    "    return len(text_chars) \n",
    "\n",
    "    \n",
    "def avg_letter_per_word(text):\n",
    "    ALPW = float(float(char_count(text))/float(len(text.split())))\n",
    "    return round(ALPW, 2)\n",
    "    \n",
    "\n",
    "def avg_letter_per_sent(text):\n",
    "    ALPS = float(float(char_count(text))/float(len(sentence_splitter(text))))\n",
    "    return round(ALPS, 2)\n",
    "    \n",
    "\n",
    "def syllable_count(text): \n",
    "    text = text.lower()\n",
    "    text = \"\".join(x for x in text if x not in exclude)\n",
    "    dic = Pyphen(lang='ru_RU')\n",
    "    count = 0\n",
    "    for word in text.split(' '):\n",
    "        word_hyphenated = dic.inserted(word)\n",
    "        count += max(1, word_hyphenated.count(\"-\") + 1)\n",
    "    return count\n",
    "    \n",
    "    \n",
    "def avg_syllab_per_word(text):\n",
    "    ASYPW = float(float(syllable_count(text))/float(len(text.split())))\n",
    "    return round(ASYPW, 2)\n",
    "    \n",
    "\n",
    "def avg_syllab_per_sent(text):\n",
    "    ASYPS = float(float(syllable_count(text))/float(len(sentence_splitter(text))))\n",
    "    return round(ASYPS, 2)    \n",
    "    \n",
    "def diffsyll(text):\n",
    "    count = 0\n",
    "    for word in text.split():\n",
    "        wrds = syllable_count(word)\n",
    "        #if wrds >= 3:\n",
    "        if wrds >= 4:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def percent_syll(text):\n",
    "    perc_diff = float(float(diffsyll(text)))/float(len(text.split()))*100\n",
    "    return round(perc_diff, 2) \n",
    "    \n",
    "      \n",
    "def get_simple_metrics(text):\n",
    "    SL = len(sentence_splitter(text))\n",
    "    WC = len(text.split())\n",
    "    ASL = avg_sentence_length(text)\n",
    "    TC = char_count(text)\n",
    "    ALPW = avg_letter_per_word(text)\n",
    "    ALPS = avg_letter_per_sent(text)\n",
    "    SYC = syllable_count(text)\n",
    "    ASYPW = avg_syllab_per_word(text)\n",
    "    ASYPS = avg_syllab_per_sent(text)\n",
    "    DW = diffsyll(text)\n",
    "    ADF = percent_syll(text)\n",
    "    return [SL, WC, ASL, TC, ALPW, ALPS, SYC, ASYPW, ASYPS, DW, ADF] \n",
    "    \n",
    "\n",
    "    \n",
    "def print_simple_metrics(text):\n",
    "    print('Количество предложений в тексте:', len(sentence_splitter(text)))\n",
    "    print('Количество слов в тексте:', len(text.split()))\n",
    "    print('Средняя длина предложений:', avg_sentence_length(text))\n",
    "    print('Количество символов в тексте:', char_count(text))\n",
    "    print('Средняя длина слова:', avg_letter_per_word(text))\n",
    "    print('Средняя длина предложений в символах:', avg_letter_per_sent(text))\n",
    "    print('Количество слогов в тексте:', syllable_count(text))\n",
    "    print('Среднее количество слогов в слове:', avg_syllab_per_word(text))\n",
    "    print('Среднее количеcтво слогов в предложении:', avg_syllab_per_sent(text))\n",
    "    print('Количество сложных слов в тексте:', diffsyll(text))\n",
    "    print('Процент сложных слов в тексте', percent_syll(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import string\n",
    "exclude = list(string.punctuation)\n",
    "\n",
    "\n",
    "def flesch_RE(text):\n",
    "    ASL = avg_sentence_length(text)\n",
    "    ASW = avg_syllab_per_word(text)\n",
    "    FRE = 206.835 - float(1.3 * ASL) - float(60.6 * ASW)\n",
    "    return round(FRE, 2)\n",
    "\n",
    "def flesch_kincaid_grade(text):\n",
    "    ASL = avg_sentence_length(text)\n",
    "    ASW = avg_syllab_per_word(text)\n",
    "    #английский язык!\n",
    "    #FKRA = float(0.39 * ASL) + float(11.8 * ASW) - 15.59\n",
    "    #русский\n",
    "    #FKRA = float(0.49 * ASL) + float(7.3 * ASW) - 16.59\n",
    "    #Оборнева\n",
    "    FKRA = float(0.5 * ASL) + float(8.4 * ASW) - 15.59\n",
    "    return round(FKRA, 2)\n",
    "    \n",
    "def smog_index(text): \n",
    "    if len(sentence_splitter(text)) >= 3:\n",
    "        SMOG = (1.043 * (30*(diffsyll(text)/len(sentence_splitter(text))))**.5) + 3.1291\n",
    "        return round(SMOG, 2)\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "        \n",
    "def coleman_liau_index(text):\n",
    "    L = round(avg_letter_per_word(text)*100, 2)\n",
    "    S = round(avg_sent_per_word(text)*100, 2)\n",
    "    CLI = float((0.058 * L) - (0.296 * S) - 15.8)\n",
    "    return round(CLI, 2)\n",
    "\n",
    "\n",
    "def dale_chall_score(text): #т.к. делаем сложные слова как 4 слога, все ок \n",
    "    word_count = len(text.split())\n",
    "    count = word_count - diffsyll(text)\n",
    "    per = float(count)/float(word_count)*100\n",
    "    difficult_words = 100-per\n",
    "    if difficult_words > 5: #дальше идет адаптация: вместо 0,0496 0,062\n",
    "        score = (0.1579 * difficult_words) + (0.062 * avg_sentence_length(text)) + 3.6365\n",
    "    else:\n",
    "        score = (0.1579 * difficult_words) + (0.062 * avg_sentence_length(text))\n",
    "    return round(score, 2)\n",
    "    \n",
    "    \n",
    "def gunning_fog(text):\n",
    "    grade = 0.4*(avg_sentence_length(text) + percent_syll(text))\n",
    "    return round(grade,2)\n",
    "       \n",
    "def print_statistics(text):\n",
    "    print('Russian Flesh reading Ease =', flesch_RE(text))\n",
    "    print('Russian Flesh-Kincaid Grade =', flesch_kincaid_grade(text))\n",
    "    print('Russian SMOG =', smog_index(text))\n",
    "    print('Russian CLI =', coleman_liau_index(text))\n",
    "    print('Russian DCH =', dale_chall_score(text))\n",
    "    print('Russian Gunning Fog =', gunning_fog(text))\n",
    "    \n",
    "def statist_vectors(text):\n",
    "    FRE = flesch_RE(text)\n",
    "    FKG = flesch_kincaid_grade(text)\n",
    "    #SMOG = smog_index(text)\n",
    "    CLI = coleman_liau_index(text)\n",
    "    DCH = dale_chall_score(text)\n",
    "    GF = gunning_fog(text)\n",
    "    return [FRE, FKG, SMOG, CLI, DCH, GF]   \n",
    "    \n",
    "    \n",
    "def statist_sum(text):\n",
    "    average = (flesch_kincaid_grade(text)+smog_index(text)+coleman_liau_index(text)+dale_chall_score(text)+gunning_fog(text))/5\n",
    "    return round(average,2)\n",
    "\n",
    "def simple_classifire(text):\n",
    "    level = statist_sum(text)\n",
    "    if level > 0 and level <13 :\n",
    "        return 1\n",
    "    if level >= 13 and level < 17:\n",
    "        return 2\n",
    "    return 3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def statist_vectors(text):\n",
    "    FRE = flesch_RE(text)\n",
    "    FKG = flesch_kincaid_grade(text)\n",
    "    SMOG = smog_index(text)\n",
    "    CLI = coleman_liau_index(text)\n",
    "    DCH = dale_chall_score(text)\n",
    "    GF = gunning_fog(text)\n",
    "    return [FRE, FKG, SMOG,CLI,DCH,GF]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем средние метрики для всех наших текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_in = texts.loc[texts['in_out'] == '1']\n",
    "df_out = texts.loc[texts['in_out'] == '0']\n",
    "df_hold = texts.loc[texts['in_out'] == 'hold'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg(lst): \n",
    "    return sum(lst) / len(lst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRE:\n",
      "48.37086857142859\n",
      "-163.74\n",
      "122.84\n",
      "FKG:\n",
      "12.157177142857142\n",
      "-0.99\n",
      "86.06\n",
      "SMOG:\n",
      "10.746830476190475\n",
      "0\n",
      "29.1\n",
      "CLI:\n",
      "15.807653333333382\n",
      "-1.45\n",
      "34.26\n",
      "DCH:\n",
      "7.050384761904762\n",
      "0.19\n",
      "16.91\n",
      "GF:\n",
      "13.212502857142871\n",
      "1.2\n",
      "71.29\n"
     ]
    }
   ],
   "source": [
    "FREs_in = []\n",
    "FKGs_in = []\n",
    "SMOGs_in = []\n",
    "CLIs_in = []\n",
    "DCHs_in = []\n",
    "GFs_in = []\n",
    "for text in df_in.text:\n",
    "    FRE = flesch_RE(text)\n",
    "    FKG = flesch_kincaid_grade(text)\n",
    "    SMOG = smog_index(text)\n",
    "    CLI = coleman_liau_index(text)\n",
    "    DCH = dale_chall_score(text)\n",
    "    GF = gunning_fog(text)\n",
    "    FREs_in.append(FRE)\n",
    "    FKGs_in.append(FKG)\n",
    "    SMOGs_in.append(SMOG)\n",
    "    CLIs_in.append(CLI)\n",
    "    DCHs_in.append(DCH)\n",
    "    GFs_in.append(GF)\n",
    "\n",
    "print('FRE:')\n",
    "print(avg(FREs_in))\n",
    "print(min(FREs_in))\n",
    "print(max(FREs_in))\n",
    "print('FKG:')\n",
    "print(avg(FKGs_in))\n",
    "print(min(FKGs_in))\n",
    "print(max(FKGs_in))\n",
    "print('SMOG:')\n",
    "print(avg(SMOGs_in))\n",
    "print(min(SMOGs_in))\n",
    "print(max(SMOGs_in))\n",
    "print('CLI:')\n",
    "print(avg(CLIs_in))\n",
    "print(min(CLIs_in))\n",
    "print(max(CLIs_in))\n",
    "print('DCH:')\n",
    "print(avg(DCHs_in))\n",
    "print(min(DCHs_in))\n",
    "print(max(DCHs_in))\n",
    "print('GF:')\n",
    "print(avg(GFs_in))\n",
    "print(min(GFs_in))\n",
    "print(max(GFs_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRE:\n",
      "48.21727826675702\n",
      "-3734.76\n",
      "144.94\n",
      "FKG:\n",
      "11.330974949221378\n",
      "-6.69\n",
      "531.41\n",
      "SMOG:\n",
      "6.323656059580239\n",
      "0\n",
      "22.08\n",
      "CLI:\n",
      "15.714939065673653\n",
      "-33.8\n",
      "363.8\n",
      "DCH:\n",
      "6.6729823967501805\n",
      "0.06\n",
      "26.03\n",
      "GF:\n",
      "12.347748815165867\n",
      "0.4\n",
      "129.6\n"
     ]
    }
   ],
   "source": [
    "FREs_out = []\n",
    "FKGs_out = []\n",
    "SMOGs_out = []\n",
    "CLIs_out = []\n",
    "DCHs_out = []\n",
    "GFs_out = []\n",
    "for text in df_out.text:\n",
    "    FRE = flesch_RE(text)\n",
    "    FKG = flesch_kincaid_grade(text)\n",
    "    SMOG = smog_index(text)\n",
    "    CLI = coleman_liau_index(text)\n",
    "    DCH = dale_chall_score(text)\n",
    "    GF = gunning_fog(text)\n",
    "    FREs_out.append(FRE)\n",
    "    FKGs_out.append(FKG)\n",
    "    SMOGs_out.append(SMOG)\n",
    "    CLIs_out.append(CLI)\n",
    "    DCHs_out.append(DCH)\n",
    "    GFs_out.append(GF)\n",
    "\n",
    "print('FRE:')\n",
    "print(avg(FREs_out))\n",
    "print(min(FREs_out))\n",
    "print(max(FREs_out))\n",
    "print('FKG:')\n",
    "print(avg(FKGs_out))\n",
    "print(min(FKGs_out))\n",
    "print(max(FKGs_out))\n",
    "print('SMOG:')\n",
    "print(avg(SMOGs_out))\n",
    "print(min(SMOGs_out))\n",
    "print(max(SMOGs_out))\n",
    "print('CLI:')\n",
    "print(avg(CLIs_out))\n",
    "print(min(CLIs_out))\n",
    "print(max(CLIs_out))\n",
    "print('DCH:')\n",
    "print(avg(DCHs_out))\n",
    "print(min(DCHs_out))\n",
    "print(max(DCHs_out))\n",
    "print('GF:')\n",
    "print(avg(GFs_out))\n",
    "print(min(GFs_out))\n",
    "print(max(GFs_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try_me = statist_vectors(train['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try_me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_me_all(texts):\n",
    "    return [statist_vectors(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = count_me_all(train['text'])\n",
    "x_test = count_me_all(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15287531 0.15497527 0.22373921 0.16671135 0.15099996 0.1506989 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(x_transform, y_train)\n",
    "# display the relative importance of each attribute\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True False False False]\n",
      "[1 1 1 4 3 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "# create the RFE model and select 3 attributes\n",
    "rfe = RFE(model, 3)\n",
    "rfe = rfe.fit(x_transform, y_train)\n",
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=2.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.001,\n",
      "          verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.58      0.65       877\n",
      "          1       0.63      0.77      0.69       797\n",
      "\n",
      "avg / total       0.68      0.67      0.67      1674\n",
      "\n",
      "[[511 366]\n",
      " [186 611]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=2.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.001,\n",
    "          verbose=0, warm_start=False)\n",
    "model.fit(x_transform, y_train)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y_test\n",
    "predicted = model.predict(x_transform_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_fit = scaler.fit(x_train)\n",
    "x_transform = scaler.transform(x_train)\n",
    "x_fit_test = scaler.fit(x_test)\n",
    "x_transform_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.74303272e+00,  5.29088121e+00,  3.67587396e+00,\n",
       "         2.51674692e+00,  2.86970401e+00,  4.82426986e+00],\n",
       "       [-7.71525017e-01,  1.06469004e+00, -1.47304268e+00,\n",
       "        -4.61482025e-02,  1.05981271e+00,  1.42129302e+00],\n",
       "       [-1.71950239e-01, -2.52765647e-01,  3.84813841e-01,\n",
       "         5.85389575e-01, -1.82099710e-02, -3.95333719e-01],\n",
       "       ...,\n",
       "       [ 1.56555625e+00, -1.19306735e+00, -2.04392083e-01,\n",
       "        -1.53198436e+00, -2.69348645e+00, -1.23237099e+00],\n",
       "       [-1.99498166e+00,  1.27307191e+00, -1.47304268e+00,\n",
       "         2.28480032e+00,  1.50981301e+00,  1.48708634e+00],\n",
       "       [-1.62048390e-02, -3.54373340e-01, -1.47304268e+00,\n",
       "        -2.18182298e-04,  2.73548462e-01, -1.35815614e-01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.47      0.60       877\n",
      "          1       0.60      0.87      0.71       797\n",
      "\n",
      "avg / total       0.71      0.66      0.65      1674\n",
      "\n",
      "[[415 462]\n",
      " [100 697]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(x_transform, y_train)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y_test\n",
    "predicted = model.predict(x_transform_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.69      0.67       877\n",
      "          1       0.63      0.58      0.61       797\n",
      "\n",
      "avg / total       0.64      0.64      0.64      1674\n",
      "\n",
      "[[606 271]\n",
      " [333 464]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# fit a k-nearest neighbor model to the data\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(x_transform, y_train)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y_test\n",
    "predicted = model.predict(x_transform_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.70      0.65       877\n",
      "          1       0.60      0.48      0.53       797\n",
      "\n",
      "avg / total       0.60      0.60      0.59      1674\n",
      "\n",
      "[[617 260]\n",
      " [415 382]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# fit a CART model to the data\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_transform, y_train)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y_test\n",
    "predicted = model.predict(x_transform_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=3.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.59      0.66       877\n",
      "          1       0.64      0.80      0.71       797\n",
      "\n",
      "avg / total       0.70      0.69      0.69      1674\n",
      "\n",
      "[[514 363]\n",
      " [159 638]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "# fit a SVM model to the data\n",
    "model = SVC(C=3.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "model.fit(x_transform, y_train)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y_test\n",
    "predicted = model.predict(x_transform_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['verbose', 'class_weight', 'tol', 'C', 'decision_function_shape', 'max_iter', 'cache_size', 'kernel', 'degree', 'probability', 'shrinking', 'coef0', 'random_state', 'gamma'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'C': array([5. , 4.9, 4.8, 4.7, 4.6, 4.5, 4.4, 4.3, 4.2, 4.1, 4. ])},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
      "0.7032010243277849\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "# prepare a range of alpha values to test\n",
    "alphas = np.array([])\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "model = SVC(C=5.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(C=alphas))\n",
    "grid.fit(x_transform, y_train)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV(cv=None, error_score='raise',\n",
      "          estimator=SVC(C=5.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False),\n",
      "          fit_params={}, iid=True, n_iter=100, n_jobs=1,\n",
      "          param_distributions={'cache_size': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001AB4CD36550>},\n",
      "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
      "          scoring=None, verbose=0)\n",
      "0.7032010243277849\n",
      "1000.0904705496463\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "# prepare a uniform distribution to sample for the alpha parameter\n",
    "param_grid = {'cache_size': sp_rand(1000)}\n",
    "# create and fit a ridge regression model, testing random alpha values\n",
    "model = SVC(C=5.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100)\n",
    "rsearch.fit(x_transform, y_train)\n",
    "print(rsearch)\n",
    "# summarize the results of the random parameter search\n",
    "print(rsearch.best_score_)\n",
    "print(rsearch.best_estimator_.cache_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=3.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.67      0.69       885\n",
      "          1       0.66      0.71      0.68       789\n",
      "\n",
      "avg / total       0.69      0.69      0.69      1674\n",
      "\n",
      "[[591 294]\n",
      " [225 564]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_svc = SVC(C=3.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "model_svc.fit(x_transform, y_train)\n",
    "print(model_svc)\n",
    "\n",
    "expected_svc = y_test\n",
    "predicted_svc = model_svc.predict(x_transform_test)\n",
    "\n",
    "print(metrics.classification_report(expected_svc, predicted_svc))\n",
    "print(metrics.confusion_matrix(expected_svc, predicted_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без SMOG\n",
    "\n",
    "SVC(C=3.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.62      0.77      0.69       885\n",
    "          1       0.65      0.48      0.55       789\n",
    "\n",
    "avg / total       0.64      0.63      0.63      1674\n",
    "\n",
    "[[683 202]\n",
    " [410 379]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без SMOG и CLI\n",
    "\n",
    "SVC(C=3.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.62      0.74      0.67       885\n",
    "          1       0.63      0.50      0.56       789\n",
    "\n",
    "avg / total       0.62      0.62      0.62      1674\n",
    "\n",
    "[[651 234]\n",
    " [396 393]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без SMOG CLI DCH GF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=90, p=2,\n",
      "           weights='uniform')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.73      0.68       885\n",
      "          1       0.63      0.52      0.57       789\n",
      "\n",
      "avg / total       0.63      0.63      0.63      1674\n",
      "\n",
      "[[645 240]\n",
      " [375 414]]\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=90, p=2,\n",
    "           weights='uniform')\n",
    "model.fit(x_transform, y_train)\n",
    "print(model)\n",
    "\n",
    "expected = y_test\n",
    "predicted = model.predict(x_transform_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier()\n",
    "model.fit(x_transform, y_train)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "А что если попробовать стандартное преобразование текстов? Традиционно делаем предобработку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_clean(s):\n",
    "    try:\n",
    "        # print(type(s))\n",
    "        clean_line = re.sub('[\\W\\d_-]+', ' ', s.lower().strip())\n",
    "        return re.sub(' +', ' ', clean_line)\n",
    "    except AttributeError:\n",
    "        ''#print (\"this was a series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_clean = train.applymap(set_clean) #применяем еще раз нашу предобработку\n",
    "test_clean = test.applymap(set_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\envs\\mllecture\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "def tfidf_vec(voc=None):\n",
    "    if(voc):\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                     stop_words='russian',\n",
    "                                     min_df=5,   \n",
    "                                     max_df=0.9)  \n",
    "        tr = vectorizer.fit_transform(train_clean[\"text\"]) \n",
    "        te = vectorizer.fit_transform(test_clean[\"text\"]) \n",
    "        return (tr, te)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer() \n",
    "        tr = vectorizer.fit_transform(train_clean[\"text\"]) \n",
    "        voc = vectorizer.get_feature_names()\n",
    "        vectorizer = CountVectorizer(vocabulary=voc) \n",
    "        te = vectorizer.fit_transform(test_clean[\"text\"]) \n",
    "        return (tr, te)\n",
    "train_counts, test_counts = tfidf_vec() \n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "x_train = tfidf_transformer.fit_transform(train_counts)\n",
    "x_test = tfidf_transformer.fit_transform(test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1674"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(x_train, y_train)\n",
    "predicted = clf.predict(x_test)\n",
    "len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.32      0.47       885\n",
      "          1       0.55      0.93      0.69       789\n",
      "\n",
      "avg / total       0.71      0.61      0.57      1674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred=predicted, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=100, p=2,\n",
      "           weights='uniform')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.62      0.66       885\n",
      "          1       0.62      0.70      0.66       789\n",
      "\n",
      "avg / total       0.66      0.66      0.66      1674\n",
      "\n",
      "[[548 337]\n",
      " [234 555]]\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=100, p=2,\n",
    "           weights='uniform')\n",
    "model.fit(x_train, y_train)\n",
    "print(model)\n",
    "\n",
    "expected = y_test\n",
    "predicted = model.predict(x_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      1.00      0.69       885\n",
      "          1       0.00      0.00      0.00       789\n",
      "\n",
      "avg / total       0.28      0.53      0.37      1674\n",
      "\n",
      "[[885   0]\n",
      " [789   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\envs\\mllecture\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_svc = SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "model_svc.fit(x_train, y_train)\n",
    "print(model_svc)\n",
    "\n",
    "expected_svc = y_test\n",
    "predicted_svc = model_svc.predict(x_test)\n",
    "\n",
    "print(metrics.classification_report(expected_svc, predicted_svc))\n",
    "print(metrics.confusion_matrix(expected_svc, predicted_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
